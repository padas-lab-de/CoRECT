<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CoRECT: Overview and Results</title>
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Google Font: Fira Sans -->
  <link href="https://fonts.googleapis.com/css2?family=Fira+Sans&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Fira Sans', sans-serif;
    }
    .plot-table {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 1rem;
    }
    .plot-item {
      min-width: 200px;
    }
    .plot-wide-2x {
      flex: 1 1 calc(40% - 1rem);
    }
    .plot-wide-2_5x {
      flex: 1 1 calc(50% - 1rem);
    }
    .subcaption {
      font-size: 0.9rem;
      text-align: center;
      margin-top: 0.25rem;
    }
    select[multiple] {
      height: auto !important;
    }
  </style>
</head>
<body class="bg-light p-4">
  <div class="container">
    <ul class="nav nav-tabs" id="myTab" role="tablist">
      <li class="nav-item" role="presentation">
        <button class="nav-link active" id="overview-tab" data-bs-toggle="tab" data-bs-target="#overview" type="button" role="tab">Overview</button>
      </li>
      <li class="nav-item" role="presentation">
        <button class="nav-link" id="results-tab" data-bs-toggle="tab" data-bs-target="#results" type="button" role="tab">Results</button>
      </li>
    </ul>
    <div class="tab-content mt-3">
      <!-- Overview Tab -->
      <div class="tab-pane fade show active" id="overview" role="tabpanel">
        <h1>CoRECT: A Framework for Evaluating Embedding Compression Techniques</h1>

        <p>
          This project accompanies our paper on evaluating embedding compression techniques for dense retrieval at scale.
          We present <strong>CoRECT</strong>, a framework designed to systematically measure the impact of compression strategies on retrieval performance.
        </p>

        <p>Our work addresses two main objectives:</p>
        <ul>
          <li><strong>Comparing embedding compression methods</strong>: We benchmark <strong>quantization</strong>, <strong>binarization</strong>, <strong>vector truncation</strong>, <strong>Principal Component Analysis</strong>, <strong>Locality-Sensitive Hashing</strong> and <strong>Product Quantization</strong>.</li>
          <li><strong>Analyzing scalability</strong>: We evaluate how retrieval quality degrades or holds up when scaling the corpus from <strong>10K to 100M passages</strong> and from <strong>10K to 10M documents</strong>.</li>
        </ul>

        <h2>CoRE: Controlled Retrieval Evaluation</h2>

        <p>
          The <strong>CoRE benchmark</strong> is a key component of our framework. It enables controlled experiments by varying corpus size and document length independently.
          Built upon <strong>MS MARCO v2</strong> and human relevance judgments (<strong>TREC DL 2023</strong>), CoRE provides:
        </p>

        <ul>
          <li><strong>Passage retrieval</strong>: 65 queries over 5 corpus sizes (10K to 100M)</li>
          <li><strong>Document retrieval</strong>: 55 queries over 4 corpus sizes (10K to 10M)</li>
        </ul>

        <p>To ensure realistic evaluation:</p>
        <ul>
          <li>Each query includes <strong>10 high-quality relevant documents</strong></li>
          <li>We adopt an advanced <strong>subsampling technique</strong> to retain <strong>hard distractors</strong>, drawn from top-ranked TREC DL system runs</li>
          <li>Each query is paired with <strong>100 mined distractors</strong> and additional random negatives</li>
        </ul>

        <p>
          This design makes CoRE robust for analyzing how retrieval performance is affected by corpus complexity and size.
        </p>

        <h2>Quantization Methods</h2>

        <p>
          We implement and benchmark a range of <strong>compression techniques</strong>, including:
        </p>

        <ul>
          <li><strong>Scalar quantization</strong>: Values are mapped into fixed bit-width ranges (e.g., 8-bit â†’ 256 bins), computed on a per-batch and per-dimension basis using <strong>percentile binning</strong> and <strong>euqal-distance binning</strong></li>
          <li><strong>Binarization</strong>: Each embedding value is converted to 0 or 1 via simple zero-thresholding as well as using the <strong>median</strong> of each dimension as a threshold.</li>
          <li><strong>Casting</strong>: We type-cast the original embedding to other floating point types, i.e. <strong>FP16</strong>, <strong>BF16</strong> or <strong>FP8</strong>, depending on the tensor type used during model training.</li>
          <li><strong>Vector Truncation</strong>: We truncate embedding vectors by taking only the first <em>x</em> dimensions, with <em>x</em> chosen according the cutoff points of our models trained with Matryoshka Representation Learning. We also combine this approach with the three methods above.</li>
          <li><strong>Principal Component Analysis</strong>: We use PCA to reduce the number of dimensions per embedding vector, choosing the same number of dimensions as for vector truncation.</li>
          <li><strong>Locality-Sensitive Hashing</strong>: We use LSH to map the embeddings to a binary vector. We choose the output length of the binary embedding such that we compress the original vector by a factor of 4, 8, 16, an 32, respectively.</li>
          <li><strong>Product Quantization</strong>: Lastly, we apply PQ, choosing six combinations for the number of subvectors and code size.</li>
        </ul>

        <h3>Implementation Details</h3>

        <ul>
          <li>Compression is applied to batches of up to <strong>50,000 vectors</strong> at once.</li>
          <li>Quantization thresholds and similar embedding-dependent parameters are computed and applied per batch.</li>
          <li>Results are shown in different plots, depending on the dataset:
            <ul>
              <li><strong>Heatmaps</strong> visualize the results that combine vector truncation with FP casting, scalar quantization (equal-distance) and binarization (zero threshold).</li>
              <li><strong>Line Charts</strong> show results on the CoRE dataset, giving insights into how compression methods perform when the difficulty of the retrieval task increases. The plots isolate vector truncation and percentile binning.</li>
              <li><strong>Pareto Plots</strong> compare all used methods at different compression ratios.</li>
            </ul>
          </li>
        </ul>

        <p>
          These methods are integrated into our open-source evaluation framework, making it easy to reproduce results and test new methods.
        </p>

        <h2>Getting Started</h2>

        <p>To run the evaluation framework, follow these steps:</p>

        <h4>1. Clone the repository</h4>

        <pre><code class="language-console">git clone https://github.com/padas-lab-de/CoRECT.git</code></pre>

        <h4>2. Install dependencies</h4>

        <p>There are two ways you can install the dependencies to run the code.</p>

        <h5>Using Poetry (recommended)</h5>

        <p>If you have the <a href="https://python-poetry.org/">Poetry</a> package manager for Python installed already, you can simply set up everything with:</p>
        <pre><code class="language-console">poetry install
source $(poetry env info --path)/bin/activate</code></pre>
        <p>After the installation of all dependencies, you will end up in a new shell with a loaded venv. In this shell, you can run the main <em>corect</em> command. You can exit the shell at any time with <em>exit</em>.</p>
        <pre><code class="language-console">corect --help</code></pre>
        <p>To install new dependencies in an existing poetry environment, you can run the following commands with the shell environment being activated:</p>
        <pre><code class="language-console">poetry lock
poetry install</code></pre>

        <h5>Using Pip (alternative)</h5>

        <p>You can also create a venv yourself and use <code>pip</code> to install dependencies:</p>
        <pre><code class="language-console">python3 -m venv venv
source venv/bin/activate
pip install .</code></pre>

        <h4>3. Run Evaluation Code</h4>

        <p>The evaluation code currently supports two datasets: A transformed version of the MS MARCO v2 dataset, called CoRE, and public BeIR datasets.
          In addition to the dataset, the code also loads an embedding model to evaluate the defined compression techniques.
          The currently supported models are
          <a href="https://huggingface.co/jinaai/jina-embeddings-v3">Jina V3</a> (jinav3),
          <a href="https://huggingface.co/intfloat/multilingual-e5-large-instruct">Multilingual-E5-Large-Instruct</a> (e5),
          <a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m">Snowflake-Arctic-Embed-m</a> (snowflake), and
          <a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0">Snowflake-Arctic-Embed-m-v2.0</a> (snowflakev2).
        </p>

        <pre><code class="language-console">corect evaluate jinav3 core     # Evaluates Jina V3 on CoRE
corect evaluate e5 beir         # Evaluates E5-Multilingual on BeIR</code></pre>

        <p>The code downloads the respective datasets from Hugging Face and uses the chosen model to generate the embeddings.
          By default, the embeddings are stored on device for later re-evaluation.
          To avoid this, change line <em>177</em> in the <code>evaluation.py</code> script to <code>None</code>.
          The embeddings will then be compressed using the compression methods specified in <code>compression_registry.py</code>.
        </p>

        <p>
          After running the evaluation code, you will find the results in the <em>results</em> folder.
          The results are stored in a JSON file in a folder structure organized by model name and dataset.
          To share the results, copy the respective JSON file to the <em>share_results</em> folder.
          Default folders for storing results and embeddings can be changed in <code>config.py</code>.
          Results are stored in the following format:
        </p>

        <pre><code class="language-json">{
    "ndcg_at_1": 0.38462,
    "ndcg_at_3": 0.33752,
    ...
    "rc_at_1000": {
        "relevant": 10.0,
        "distractor": 99.63077
    },
    "rankings": {
        "qid1": {
            "relevant": {"cid1": 0, "cid9": 5, ...},
            "distractor": {"cid3": 2, "cid5": 3, ...},
            "random": {"cid17": 1, "cid15": 11, ...}
        },
        ...
    }
}</code></pre>

        <h4>4. Extend CoRECT</h4>

        <h5>Add New Compression Technique</h5>
        <p>
          The currently implemented compression techniques can be found in the
          <a href="src/corect/quantization">quantization</a> folder.
          To add a new method, implement a class that extends
          <a href="src/corect/quantization/AbstractCompression.py">AbstractCompression</a>
          and add your custom compression technique via the <code>compress()</code> method:
        </p>

        <pre><code class="language-python">import torch
from corect.quantization.AbstractCompression import AbstractCompression

PRECISION_TYPE = {
    "float16": 16,
    "bfloat16": 16,
}

class FloatingCompression(AbstractCompression):
    def __init__(self, precision_type: str = "float16"):
        assert precision_type in PRECISION_TYPE
        self.precision_type = precision_type

    def compress(self, embeddings: torch.Tensor) -> torch.Tensor:
        if self.precision_type == "float16":
            return embeddings.type(torch.float16)
        elif self.precision_type == "bfloat16":
            return embeddings.type(torch.bfloat16)
        else:
            raise NotImplementedError(
                f"Cannot convert embedding to invalid precision type {self.precision_type}!"
            )</code></pre>

        <p>
          To include your class in the evaluation, modify the <code>add_compressions</code> method in the
          <a href="src/corect/compression_registry.py">compression registry</a>
          to register your class with the compression methods dictionary:
        </p>

        <pre><code class="language-python">from typing import Dict
from corect.quantization.AbstractCompression import AbstractCompression
from corect.quantization.FloatCompression import PRECISION_TYPE, FloatCompression

class CompressionRegistry:
    _compression_methods: Dict[str, AbstractCompression] = {}

    @classmethod
    def get_compression_methods(cls) -> Dict[str, AbstractCompression]:
        return cls._compression_methods

    @classmethod
    def clear(cls):
        cls._compression_methods.clear()

    @classmethod
    def add_baseline(cls):
        cls._compression_methods["32_full"] = FloatCompression("full")

    @classmethod
    def add_compressions(cls):
        # Add your compression method here to use it for evaluation.
        for precision, num_bits in PRECISION_TYPE.items():
            cls._compression_methods[f"{num_bits}_{precision}"] = FloatCompression(precision)</code></pre>

        <p>
          You should now be able to evaluate your compression technique by running the evaluation script as described above.
        </p>

        <h5>Add New Model</h5>

        <p>
          New embedding models can be added by implementing the
          <a href="src/corect/model_wrappers/AbstractModelWrapper.py">AbstractModelWrapper</a>
          class, which requires implementing encoding functions for queries and documents.
          Any model available via <code>transformers</code> can be added easily. For reference, consider the example below:
        </p>

        <pre><code class="language-python">from typing import List, Union
import torch
from transformers import AutoModel, AutoTokenizer
from corect.model_wrappers import AbstractModelWrapper
from corect.utils import cos_sim

def _last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

class Qwen3Wrapper(AbstractModelWrapper):
    def __init__(self, pretrained_model_name="Qwen/Qwen3-Embedding-0.6B"):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(pretrained_model_name, trust_remote_code=True, torch_dtype=torch.float16)
        self.encoder.cuda()
        self.encoder.eval()
        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True, padding_side='left')

    def _encode_input(self, sentences: List[str]) -> torch.tensor:
        inputs = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt', max_length=8192)
        inputs.to('cuda')
        model_outputs = self.encoder(**inputs)
        outputs = _last_token_pool(model_outputs.last_hidden_state, inputs['attention_mask'])
        outputs = torch.nn.functional.normalize(outputs, p=2, dim=1)
        return outputs

    def encode_queries(self, queries: List[str], **kwargs) -> torch.tensor:
        return self._encode_input(queries)

    def encode_corpus(self, corpus: Union[str, List[str]], **kwargs) -> torch.tensor:
        if isinstance(corpus, str):
            corpus = [corpus]
        return self._encode_input(corpus)

    def similarity(self, embeddings_1: torch.Tensor, embeddings_2: torch.Tensor) -> torch.Tensor:
        return cos_sim(embeddings_1, embeddings_2)

    @property
    def name(self) -> str:
        return "Qwen3Wrapper"</code></pre>

        <p>
          The wrapper then needs to be registered in the <code>get_model_wrapper()</code> method of the
          <a href="src/corect/cli/evaluate.py">evaluation script</a>:
        </p>

        <pre><code class="language-python">from typing import Tuple
from corect.model_wrappers import AbstractModelWrapper, JinaV3Wrapper, Qwen3Wrapper

def get_model_wrapper(model_name: str) -> Tuple[AbstractModelWrapper, int]:
    if model_name == "jinav3":
        return JinaV3Wrapper(), 1024
    elif model_name == "qwen3":
        return Qwen3Wrapper(), 1024     # 1024 is the embedding dimension of the Qwen model.
    else:
        raise NotImplementedError(f"Model {model_name} not supported!")</code></pre>

        <p>The model can then be evaluated as follows:</p>
        <pre><code class="language-console">corect evaluate qwen3 core</code></pre>

        <h5>Add New Dataset</h5>

        <p>
          Our framework supports the addition of any HuggingFace retrieval datasets with corpus, queries and qrels splits.
          To add a custom dataset, navigate to the
          <a href="src/corect/dataset_utils.py">dataset utils</a> script, add a load function for your new dataset and register it in the <code>load_data()</code> function.
          You also need to add information on the new dataset to the <code>datasets</code> dictionary in this class in the form of
          <code>datasets[&lt;dataset_name&gt;]=[&lt;dataset_name&gt;]</code>. The example below adds a new dataset called <em>my_ir_dataset</em>:
        </p>

        <pre><code class="language-python">from collections import defaultdict
from typing import Dict, Tuple
from datasets import load_dataset

CoRE = {"passage": {"pass_core": 10_000, "pass_10k": 10_000}}
DATASET = ["my_dataset_name"]
CoRE_NAME = "core"
DATASET_NAME = "my_ir_dataset"
DATASETS = {CoRE_NAME: CoRE, DATASET_NAME: DATASET}

def _load_core_data(dataset_sub_corpus: str):
    # Code for loading CoRE
    ...

def _load_my_dataset(dataset_name: str) -> Tuple[defaultdict, Dict[str, str], defaultdict, defaultdict]:
    dataset_queries = load_dataset(f"hf_repo/my_dataset", "queries")
    dataset_qrels = load_dataset(f"hf_repo/my_dataset", "default")
    dataset_corpus = load_dataset(f"hf_repo/my_dataset", "corpus")

    qrels = defaultdict(dict)
    for q in dataset_qrels:
        query_id = q["query-id"]
        corpus_id = q["corpus-id"]
        qrels[query_id][corpus_id] = int(q["score"])

    queries = {q["_id"]: q["text"] for q in dataset_queries["queries"] if q["_id"] in qrels.keys()}

    corpora = defaultdict(dict)
    for d in dataset_corpus["corpus"]:
        corpora[dataset_name][d["_id"]] = {"title": d["title"], "text": d["text"]}

    return corpora, queries, qrels, qrels

def load_data(dataset_name: str, dataset_sub_corpus: str):
    if dataset_name == CoRE_NAME:
        return _load_core_data(dataset_sub_corpus)
    elif dataset_name == DATASET_NAME:
        return _load_my_dataset(dataset_sub_corpus)
    else:
        raise NotImplementedError(f"Cannot load data for unsupported dataset {dataset_name}!")</code></pre>

        <p>
          Running the evaluation script on the new dataset can then be achieved by executing the following command:
        </p>
        <pre><code class="language-console">corect evaluate jinav3 my_ir_dataset</code></pre>
      </div>

      <!-- Results Tab -->
      <div class="tab-pane fade" id="results" role="tabpanel">
        <p>Please select from the options below to view the plots.</p>
        <div class="row mb-4">
          <div class="col-md-3">
            <label class="form-label">Dataset</label>
            <select class="form-select" id="dataset">
              <option value="core">CoRE</option>
              <option value="beir">BEIR</option>
            </select>
          </div>
          <div class="col-md-3">
            <label class="form-label">Metric</label>
            <select class="form-select" id="metric">
              <option value="ndcg_at_10">NDCG@10</option>
              <option value="recall_at_100">Recall@100</option>
              <option value="recall_at_1000">Recall@1000</option>
            </select>
          </div>
          <div class="col-md-3">
            <label class="form-label">Model</label>
            <select class="form-select" id="models" multiple size="4">
              <option selected>Jina v3</option>
              <option>Multilingual E5 (Large Instruct)</option>
              <option>Snowflake v1</option>
              <option>Snowflake v2</option>
            </select>
          </div>
          <div class="col-md-3">
            <label class="form-label">Visualization</label>
            <select class="form-select" id="visualization">
              <option value="heatmaps">Heatmaps</option>
              <option value="line_charts">Line charts</option>
              <option value="pareto_plots">Pareto Plots</option>
            </select>
          </div>
        </div>

        <div id="plotContainer">
          <!-- Plots will appear here -->
        </div>
      </div>
    </div>
  </div>

  <!-- Bootstrap JS -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    const datasetSelect = document.getElementById('dataset');
    const visualizationSelect = document.getElementById('visualization');
    const modelSelect = document.getElementById('models');
    const metricSelect = document.getElementById('metric');
    const plotContainer = document.getElementById('plotContainer');

    [datasetSelect, visualizationSelect, modelSelect, metricSelect].forEach(el => el.addEventListener('change', renderPlots));

    function sanitizeModel(model) {
      switch (model) {
        case 'Jina v3':
          return 'jina';
        case 'Multilingual E5 (Large Instruct)':
          return 'e5';
        case 'Snowflake v1':
          return 'snowflake';
        case 'Snowflake v2':
          return 'snowflakev2';
        default:
          return 'jina'; // fallback to default
      }
    }

    function renderPlots() {
      const dataset = datasetSelect.value;
      const viz = visualizationSelect.value;
      const metric = metricSelect.value;
      const selectedModels = Array.from(modelSelect.selectedOptions).map(o => o.value);
      if (selectedModels.length === 0) {
        selectedModels.push('Jina v3');
        modelSelect.options[0].selected = true;
      }
      plotContainer.innerHTML = '';

      const makePlot = (src, caption, extraClass = '') => `
        <div class="plot-item ${extraClass}" title="${caption}">
          <embed src="${src}" width="100%"/>
          <div class="subcaption">${caption}</div>
        </div>`;

      selectedModels.forEach(model => {
        const modelDir = sanitizeModel(model);
        const modelHeader = `<h5 class="mt-4">Model: ${model}</h5>`;
        let content = '';

        if (viz === 'heatmaps') {
          const dataset_name = dataset === 'core' ? 'CoRE' : 'BEIR';
          content += '<p>Performance on ' + dataset_name + ' combining different levels of dimensionality reduction (y-axis) with different levels of precision/quantization (x-axis).</p>'
          if (dataset === 'core') {
            let src1 = [];
            let src2 = [];
            for (let i = 0; i <= 4; i++) {
              src1.push(`./resources/heatmaps/${modelDir}/${metric}/passage_${10000*(10**i)}.png`);
              src2.push(i <= 3 ? `./resources/heatmaps/${modelDir}/${metric}/document_${10000*(10**i)}.png` : null);
            }
            content += '<div class="plot-table mt-3">' + src1.map((src, i) => makePlot(src, `Passage corpus size=${10000*(10**i)}`)).join('') + '</div>';
            content += '<div class="plot-table">' + src2.map((src, i) => src ? makePlot(src, `Document corpus size=${10000*(10**i)}`) : '').join('') + '</div>';
          } else if (dataset === 'beir') {
            const batch1 = [`./resources/heatmaps/${modelDir}/${metric}/aggregated.png`];
            const dataset_names = ['ArguAna', 'CQADupstack', 'DBPedia', 'FiQA-2018', 'NFCorpus', 'NQ', 'Quora', 'SCIDOCS', 'SciFact', 'Touche-2020', 'TREC-COVID']
            const datasets = ['arguana', 'cqadupstack',  'dbpedia', 'fiqa', 'nfcorpus', 'nq', 'quora', 'scidocs', 'scifact', 'touche2020', 'trec-covid',];
            const batch2 = datasets.map(name => `./resources/heatmaps/${modelDir}/${metric}/${name}.png`);

            content += '<div class="plot-table mt-3">' + makePlot(batch1[0], 'Average performance and standard deviation on BEIR datasets with absolute performance delta to full precision (upper left corner)') + batch2.map((src, i) => makePlot(src, dataset_names[i])).join('') + '</div>';
          }
        } else if (viz === 'line_charts' && dataset === 'core') {
          content += '<p>Performance on the CoRE dataset with:<br> a) full-precision, i.e. 32-bit floating point vectors, passage retrieval reducing dimensionality, i.e. only taking the first 32 vector dimensions,<br> b) passage retrieval with all vector dimensions but different levels of quantization, and<br> c) document retrieval with all vector dimensions but different levels of quantization.</p>'
          const plots = [
            { file: `./resources/line_charts/${modelDir}/${metric}/combined.png`, caption: '', width: 'plot-wide-2_5x' }
          ];
          content += '<div class="plot-grid">' + plots.map(p => makePlot(p.file, p.caption, p.width)).join('') + '</div>';
        }

        plotContainer.innerHTML += modelHeader + content;
      });
    }

    function updateVisualizationOptions() {
      const selectedDataset = datasetSelect.value;
      const lineOption = visualizationSelect.querySelector('option[value="line_charts"]');
      const paretoOption = visualizationSelect.querySelector('option[value="pareto_plots"]');
      if (selectedDataset === 'beir') {
        if (visualizationSelect.value === 'line_charts') {
          visualizationSelect.value = 'heatmaps';
        }
        lineOption.disabled = true;
        paretoOption.disabled = false;
      } else {
        if (visualizationSelect.value === 'pareto_plots') {
          visualizationSelect.value = 'heatmaps';
        }
        lineOption.disabled = false;
        paretoOption.disabled = true;
      }
    }

    datasetSelect.addEventListener('change', () => {
      updateVisualizationOptions();
      renderPlots();
    });

    updateVisualizationOptions();

    renderPlots();
  </script>
</body>
</html>
